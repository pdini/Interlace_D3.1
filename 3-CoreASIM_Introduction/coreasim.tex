\chapter{Introduction to the CoreASIM Language, Interpreter, and ICEF}
\label{ch:CoreAsimIntro}

\vspace{-1cm}
\begin{center}
Eduard Hirsch
\end{center}

In this chapter an overview of the used technologies for running the INTERLACE Specification is given. At the beginning a quick start description is provided in order to jump right into the execution of the model.

Further details which are enabling the Abstract State Interacting Machines (ASIM) specifications to be executed are discussed and why this possible in a simple and stable manner.

\section{Quick Start Vagrant}
\label{sec:quick-start-vagrant}

There are two base environments available. One based one docker and one based on vagrant. The focus shifted from the vagrant environment which can be downloaded at github\footnote{https://github.com/InterlaceProject/ASIMVagrantEnvironment} to a docker based version which is explained in section \ref{sec:quick-start-docker}. Nevertheless, for developer preferring vagrant, the setup will be still explained.

The vagrant definition provides a running environment for executing the INTERLACE ASIM definitions. During the provisioning process an ubuntu vagrant box is set up which installs the necessary components on that box. It is cloning and building the ICEF framework\footnote{https://github.com/biomics/icef} as well as the ASIM Specification\footnote{https://github.com/InterlaceProject/ASIMSpec} into the data directory where it is finally ready for usage.

\subsection{Prerequisites}

Download and install the following software products:

\begin{itemize}
	\item Virtual Box: https://www.virtualbox.org/
	\item git: https://git-scm.com/downloads
	\item vagrant: https://www.vagrantup.com/
\end{itemize}

\subsection{Clone Environment}

To clone the ASIM vagrant environment from github into a directory git can be utilized:

\begin{lstlisting}
	git clone https://github.com/InterlaceProject/ASIMVagrantEnvironment.git
\end{lstlisting}

\subsection{Execution}

Once all software components are installed and the vagrant definitions are cloned it is possible to call

\begin{lstlisting}
	execute.sh
\end{lstlisting}

from the main directory in order to let the INTERLACE specifications run. Note: when using Windows it is necessary to start that command within git-bash which needs to run in elevated admin mode (right click $\rightarrow$ start as administrator).

On the very first execution the script is provisioning a virtual machine based on ubuntu by calling \texttt{vagrant up} which may take some time. Consecutive calls will be much faster. A detailed description explaining the precise process is covered in section \ref{sec:env-exec}.

Once the execution is started it will run until it is stopped by pressing \textbf{ctrl + c} or by calling

\begin{lstlisting}
	stop.sh
\end{lstlisting}

from any other console window.

\section{Quick Start Docker}
\label{sec:quick-start-docker}

The docker project at github\footnote{https://github.com/InterlaceProject/ASIMDockerEnvironment} is also based on virtualization like the vagrant environment but emphasizing \textit{Operating System} instead of \textit{Hardware virtualization}\footnote{https://www.docker.com/what-container\#comparing}.

\subsection{Prerequisites}

\begin{itemize}
	\item install docker
	\item install git (including git bash for windows)
\end{itemize}

On Linux machines it is important to add the current user to the docker group in order to manage docker container and images. Otherwise all further explained commands need to be executed as root or with sudo.

For Windows machines use \textit{git-bash} to execute the commands described in the following sections.

\subsection{Before First Execution}

In order to configure the environment it is necessary to call the following script:

\begin{lstlisting}
	./configure
\end{lstlisting}

This will generate a docker container image called \textit{asim} where all the necessary frameworks are build and prepared for execution of the specifications. The ICEF framework as well as the ASIM model specifications are cloned outside of the container to simplify development.

\subsection{Execute Specification}

The container image \textit{asim} created during the configuring step can be started by calling

\begin{lstlisting}
	./execute
\end{lstlisting}

A container started in this way is called \textit{active\_asim} and running all the necessary steps like starting an ICEF manager as well a ICEF brapper to run the ASIM specifications.

Like the vagrant environment a running execution may be stopped by pressing \textbf{ctrl + c}.

\section{Execution Environment Stack}
\label{sec:exec-env-stack}

Independent of the used virtualization techniques a consistent base system is used. So both docker as well as vagrant are provisioning a Linux based operating system using an Ubuntu 16.04 LTS (Long Term Support) distribution.
That consistent, stable and reliable structure will be important later when considerations about provability as well testability come into place. That design provides always the same preconditions and everybody executing or testing against the specifications will receive the same results.

\subsection{Software Stack}
\label{sec:env-exec-stack-software-stack}

The Ubuntu 16.04 LTS distribution is enhanced and updated according to the needs of an ASIM executing machine as well as to the needs of developers working with that virtual system. To be more specific the following components are installed during the provisioning process:

\begin{itemize}
	\item curl $\rightarrow$ Tool for querying REST resources (used for downloading package resources)
	\item nodejs $\rightarrow$ JavaScript engine including the package manager npm (running the Manager Component of ICEF)
	\item build-essential $\rightarrow$ Packages needed to compile a debian based package (used for building the project sources)
	\item maven $\rightarrow$ Java build and packaging tool (used for building the project sources)
	\item vim $\rightarrow$ Well known U/Linux editor (for development purposes)
	\item git $\rightarrow$ Distributed Version Control System (downloading source repositories from GitHub)
	\item Java 8 $\rightarrow$ Programming Language used for coreASIM base system implementation (running ASIM instances)
\end{itemize}

\subsection{Provisioning Process}

The provisioning process can be separated into 3 steps which are executed when \textit{.\/configure} for docker or \textit{vagrant up} for vagrant is called from the command line:

\begin{enumerate}
	\item Download ASIMSpec and ICEF from GitHub
	\item Install of software packages mentioned in section \ref{sec:env-exec-stack-software-stack}
	\item Build ICEF framework and prepare virtual machine for execution
\end{enumerate}

For development purposes the ASIMSpec as well as the ICEF framework are cloned into directories which are available from the host machine and the virtual guest machine. This is necessary because then it is possible to directly edit the source or debug from outside and execute the code from within the container. Consequently it is not necessary to copy the code into the container when changes where done.

For \textbf{Vagrant} shared folder is configured over the vagrant file

\begin{lstlisting}
	config.vm.synced_folder "./data", "/vagrant-data"
\end{lstlisting}

saying that on the host system the folder \textit{data} is used and on the guest system a folder \textit{vagrant-data} is mounted which are shared and thus containing the same content. Additionally during provisioning a symbolic link in the home directory is created called project (\textit{/home/ubuntu/project}) which is linking the mounted root folder /vagrant-data.

When the virtual machine is stopped the data directory on the host is kept and can be still manipulated or executed (of course only if the framework dependencies are installed on the host as well).

For \textbf{Docker} we need to first clone all GitHub sources to the host machine and can only then share the files into the guest container by using the command line option "-v"

\begin{lstlisting}
	docker run -v "$1/ASIMSpec:/home/ASIMSpec" \
	           -v "$1/icef:/home/icef" \
	           --name active_asim -it asim /$2
\end{lstlisting}

This line is part of the script \textit{scripts/runDocker.sh}. \$1 here stands for the local directory of the environment and \$2 will be explained later in section \ref{sec:env-exec-stack-exec-stack}. Important to note is that the \textit{ASIMSpec} folder of the host machine is shared into \textit{/home/ASIMSpec} of the docker container and the \textit{icef} directory is shared into \textit{/home/icef}.

\subsection{Execution}
\label{sec:env-exec-stack-exe}

The execution stack in figure \ref{fig:icef-intro-asim} shows where a ICEF specification is transmitted to for it to be executed. The details will be covered in section \ref{sec:icef-intro}. In this part of the document, however, we will take a look at how and which services are started.

Due to limitations of the ICEF framework it is currently not possible to cleanly shut down a running ICEF simulation with several running ASIM instances. Therefore both environments have to start and stop all services for every execution in order to grant a correct execution set-up.

\subsubsection{Start Services Processes}: Both environments offer a script (\textit{execute(.sh)}) to be called from the host system and one from within the virtualized machine (\textit{executeASIMSpec.sh/executeOnGuest.sh}). Whereas the script on the virtualized system only differ in directory references the outside scripts are need to handle different things as one script is dealing with docker and the other with vagrant.

In detail Docker can immediately start the servers and run the script inside the container whereas vagrant needs add an additional step if the virtual machine is not up and running yet. Thus vagrant

\begin{enumerate}
	\item is trying to start the server processes and submit the specifications
	\item if the first step failed is checking for the virtual machine if it is running and if not it tries to restart the virtual machine
	\item if the restart has been successful the first step will be retried.
\end{enumerate}

When now focusing on the first step which is basically the same on both environments it can be discussed in detail how the process is continued. So on the guest system we are first running a so called \textit{CASIMA} which is short hand for coreASIM-Manager \ref{sec:icef-intro}. This manager takes care about ASIM states, scheduling and also acts as messaging backbone.

Next a second service process is started. The service is a wrapper for the coreASM implementation and its name is Brapper (BIOMICS wrapper). This Brapper service executes enhanced ASM code named BSL that offers additional language primitives specifically designed during the BIOMICS project. Those features mainly include interaction features used to model biochemical systems, nevertheless, and actually because of that they are also suitable for decentralized and distributed computer system.

When a Brapper is going to be started it needs to be register at a manager instance. As described in section \ref{sec:icef-intro} it is possible to start multiple Brappers. Once a simulation is submitted to the manager, the manager is distributing the different simulations including their ASIMs to different Brapper services for executing them there trying to spread the load equally.

For the sake of simplicity the environment starts only one Brapper by default. In the final implementation it'll be possible to configure the number of Brappers used for execution.

\subsubsection{Submit ICEF definition file}: Finally after the service processes are running it is possible to submit a specification file to the manager. This is done by the executing bash script which is calling a  nodejs client:

\begin{lstlisting}
	...
	node loadICEF.js $project/ASIMSpec/run.icef localhost 9090
	...
\end{lstlisting}

The above listing uses a \textit{\$project} variable containing the path of ASIMSpec to find the icef specification. By convention the icef definition file of the ASIM Specifications is called \textit{run.icef} to clearly identify the entry point for the environment.
The last two parameters for the script are host as well as port where the manager services is running and waiting for requests.

\subsection{Development}
\label{sec:development}

For developing new simulations it is important to have a proper development environment. There are existing many IDE/Tool/Debuggers for Java and JavaScript which are used for Brapper, Manager or coreAS(I)M plugin development but there are only limited IDE choices for implementing AS(I)Ms.

For the public Integrated Development Environment Eclipse, developers of the ICEF framework have adopted the coreASM plugin for supporting the additional language primitives. Thus for working with ASIM implementations of INTERLACE you ideally DO NOT download the coreASM package which is available at the market place of Eclipse. The adopted plugin working with the new language elements is provided by the ICEF framework but is needed to build and imported manually into Eclipse.

\textbf{Import the IDE Eclipse plugin for coreASIM}
\todo{rewrite import - quick info}
1) Build Framework

You can call

./execute /bin/bash

to enter the container and do the next steps from "inside" or if you've got maven installed you can do them directly on your host from "outside".

So you'd need 

%cd icef/coreASIM/org.coreasim.parent/ && mvn package install

which copies the engine and util jars then

%cd icef/coreASIM/org.coreasim.eclipse && mvn package install
   
to build the packages and copy the jar files for the plugin automatically.


2) Setup Eclipse

* Install the Eclipse PDE (Plug-in Development Environment) from the marketplace for your Eclipse installation (restart)
* Then import the Eclipse project from icef/coreASIM/org.coreasim.eclipse    (File -> Import: Existing Project into Workspace)
 before actually importing the plugin you could try if everything works by starting a second eclipse instance including the ASIM plugin by right clicking the project -> Run As -> Eclipse Application
* Finally you'd need to import the plugin by going to the export wizard: File/Export -> There look for export "Deployable plug-ins and fragments", select the compiled plug-in fragment org.coreasim.eclipse, and then in the lower half of the page select "Install into host"-> finish.


\section{ICEF - The Interaction Computing Execution Framework}
\label{sec:icef-intro}

The interaction framework is wrapping the original coreASM framework in order to extend it and giving it the capabilities to execute concurrent and distributed. It has been developed in a project called BIOMICS and financed by the European Commission.

\textbf{This wrapping took place on three levels:}

\textbf{First} the interpreter coreASM had to be extended supporting additional languages primitives as well as communications features. Here BSL replaces ASM as a new language having a new interpreter coreASIM.

\textbf{Second} a place has been created were the now so called Abstract State Interaction Machines (ASIMs) take over and are able to execute in parallel. This environment is called Brapper (short for BIOMICS Wrapper).

\textbf{Third} a central server called manager takes care handling distributed Brapper instances dealing with message and scheduling issues.

Technology wise coreASIM changes the coreASM implementation in Java. Brappers are written in Java as well but the managers are coordinating the Brappers using nodejs and therefor written in JavaScript.

\subsection{Framework Stack}

Next, the framework stack of ICEF in terms of the INTERLACE implementation is described more detailed. In figure \ref{fig:icef-intro-asim} the stack can be viewed in different levels of granularity.

As a stable base for the INTERLACE execution environment stack a Linux based system, Ubuntu 16.04 LTS, has been chosen. A LTS (Long Term Support) version is chosen to be sure to have a reliable platform which is maintained by the distributor for long time. Ubuntu publishes those version every two years and promises a maintenance duration of five years\footnote{https://www.ubuntu.com/info/release-end-of-life}.

In listing \ref{lst:icef-json-spec} it is possible to see how the specification for the simulation is looking like. Side note: A definition for the client here is not given because the client which is sending credit, debit or other requests is spawned and destroyed on demand by the scheduler ASIM.

\begin{lstlisting}[language=json,firstnumber=1,caption={ICEF Json Specification for Interlace},captionpos=b,label=lst:icef-json-spec]
{
   "id": "interlace", 
   "schedulers": [
       {
	   "file": "casim/scheduler.casim",
	   "start": true
       }
   ], 
   "asims": [       
       {
           "file": "casim/server.casim",
           "start": true
       }
   ]
}
\end{lstlisting}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth, clip, trim=1mm 1mm 1mm 1mm]{Figures/environment_asim}
  \caption{ASIM Execution Environment Overview}
  \label{fig:icef-intro-asim}
\end{figure}


\section{coreASIM}

\section{Interpreter}

\section{INTERLACE Model Execution Environment}
\label{sec:env-exec}

\section{keep for later usage}
\todo{section remove later}
What will happen:
\begin{itemize}
	\item On the very first execution the script is provisioning a virtual machine based on ubuntu by using vagrant up
	\item If the virtual machine is not yet running it tries to start the virtual machine.
	\item If the VM is finally running the script data/execute.sh is called on the guest vm.
\end{itemize}

The actual execution is running inside of the Virtual Machine:

The script data/execute.sh starts the icef manager as well as one brapper. Then it is submitting the specification (run.icef) located in data/ASIMSpec/ to the manager. When done the script is waiting for a stop command or may also be stopped using \textbf{ctrl + c}.

\todo{sections structure, text}